Optimizing MongoDB queries is essential for improving performance, reducing execution time, and minimizing resource . 
Here are some key strategies:


1. Use Indexes (Most Important üöÄ)
Indexes speed up read operations by allowing MongoDB to find data quickly.

‚úÖ Check if Indexes are Used
db.collection.find({ field: "value" }).explain("executionStats")

If `IXSCAN` appears in `executionStats`, MongoDB is using an index. Otherwise, it‚Äôs performing a **collection scan** (`COLLSCAN`), which is slow.

### ‚úÖ **Create Indexes for Frequently Queried Fields**
```js
db.collection.createIndex({ fieldName: 1 }) // 1 for ascending, -1 for descending
```
Example:
```js
db.users.createIndex({ email: 1 }) // Indexing email field
```

---

2. Use Covered Queries
A **covered query** means MongoDB can fetch all required fields from the index itself, without accessing the actual document.

db.users.find({ email: "test@example.com" }, { email: 1, _id: 0 })

Ensure this index exists:

db.users.createIndex({ email: 1 })

This way, MongoDB does not need to fetch the document.

---

3. Optimize Query Patterns
| ------------------------------- | ------------------------------------------------------------------------------------- |
| ‚úÖ **Field/Operator**            | üöÄ **Why Use It?**                                                                    |
| ------------------------------- | ------------------------------------------------------------------------------------- |
| **`$in` (If Indexed)**          | Efficient **when used with indexed fields**. Reduces the number of scanned documents. |
| **`$gte / $lte`**               | Supports **index scans**, making queries much faster.                                 |
| **`$eq`**                       | Uses **indexes effectively**, improving query speed.                                  |
| **`$and`**                      | Works well with **indexed fields** and compound indexes.                              |
| **`$text`** (With Index)        | Highly optimized for **search queries** when a **text index** is used.                |
| **Compound Indexes**            | Indexes on **multiple fields** improve performance instead of using `$or`.            |
| **Covered Queries**             | Queries where **all fields are indexed** (avoids fetching from disk).                 |
| **Projection (`{ field: 1 }`)** | Fetching **only required fields** reduces **I/O and memory usage**.                   |
| ------------------------------- | ------------------------------------------------------------------------------------- |


4. Don't use it: 
| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| ‚ùå **Field/Operator**              | ‚ö† **Why Avoid It?**                                                                                                       |
| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| **`$or`**                          | Can cause **collection scans** if fields aren‚Äôt indexed properly. Use **compound indexes** instead.                        |
| **`$where`**                       | Executes **JavaScript** on the server, making it **slow and unoptimized**. Use **aggregation or indexed queries** instead. |
| **`$ne` (Not Equal)**              | **Prevents index usage**, leading to full collection scans. Use **range queries** (`$gte`, `$lte`) instead.                |
| **`$nin`**                         | Like `$ne`, it **disables index scans** and forces a full collection scan.                                                 |
| **`$not`**                         | Works like `$ne` and causes **index inefficiency**. Try filtering differently.                                             |
| **`$exists: false`**               | MongoDB cannot use an index when checking for missing fields.                                                              |
| **`$regex`** (Without Index)       | Causes **slow full collection scans** unless the search term is **anchored (`^pattern`) and indexed**.                     |
| **`$elemMatch`** (On Large Arrays) | May cause performance issues if used on **large embedded arrays**. Index **subfields instead**.                            |
| **Sorting Without Index**          | Sorting **without an index** can cause high memory usage. Always **create an index on the sorted field**.                  |
| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------- |



---

4.Limit Data Fetching:

‚úÖ Use Projection to Fetch Only Required Fields
db.users.find({}, { name: 1, email: 1, _id: 0 }) // Only returns name and email

‚úÖ Use `limit()` and `skip()` for Pagination
Instead of fetching all results:

db.users.find().limit(10).skip(20)

## **5. Avoid Large `$lookup` (Joins)**
If using **MongoDB Aggregation**, avoid excessive `$lookup` operations. Instead:
- Use **denormalization** (storing related data inside a document).
- Use **referencing** if data duplication is a concern.

---

6. Use Caching for Frequent Queries
- Store frequently accessed data in **Redis** to avoid repetitive MongoDB reads.

---

7. Monitor Queries & Performance**
‚úÖ **Use the MongoDB Profiler**
```js
db.setProfilingLevel(2) // Log all queries
db.system.profile.find().sort({ ts: -1 }).limit(5) // View slowest queries
```

Monitoring Queries & Performance in MongoDB Atlas
1Ô∏è‚É£ Performance Advisor (Identify Slow Queries)
üìå Location:
Go to MongoDB Atlas Dashboard ‚Üí Select Your Cluster ‚Üí Click on Performance Advisor
üîç What It Does:
Detects slow queries and suggests index optimizations
Shows query execution time and collection scans
Provides index recommendations for performance improvement


2Ô∏è‚É£ Query Profiler (Monitor Real-Time Queries)

3Ô∏è‚É£ Real-Time Metrics (Monitor CPU, Memory, and Disk Usage)
4Ô∏è‚É£ explain() Method (Check Index Usage for Queries)
5Ô∏è‚É£ Slow Query Logs (Set a Logging Threshold)



‚úÖCheck Query Execution Time 

db.collection.find({ field: "value" }).explain("executionStats")

Look for `executionTimeMillis` to find slow queries.

---

## **Conclusion**
### üöÄ **Best Practices Summary**
‚úÖ **Use Indexes** for frequently queried fields  
‚úÖ **Optimize Query Structure** (avoid unnecessary `$or` queries)  
‚úÖ **Fetch Only Needed Data** using projections  
‚úÖ **Use Pagination** (`limit()`, `skip()`)  
‚úÖ **Monitor Slow Queries** with `explain()` and profiling  

By following these optimizations, **MongoDB queries will run much faster and more efficiently!** üöÄ Let me know if you need help with a specific query. üòä




---------------------------------------------------------------------------------------------



Performance, fault tolerance, Deployment
---

1. what influences Performance?
2. Capped Collections
3. Replica Sets
4. Sharding
5. MongoDB Server Deployment




1Ô∏è‚É£. what influences Performance?
--------------------------------

Performance in MongoDB (or any database system) is influenced by a variety of factors, ranging from hardware and configuration to query design and data structure. 
Below is a detailed breakdown of the key factors that influence MongoDB performance:

1. Hardware Resources
CPU: MongoDB performance can be CPU-bound, especially for complex queries, aggregations, or indexing operations.
RAM: MongoDB relies heavily on memory for caching frequently accessed data and indexes. Insufficient RAM can lead to increased disk I/O, slowing down performance.
Disk Speed: Disk I/O is a critical factor. Faster disks (e.g., SSDs) improve read/write performance, especially for large datasets.
Network: In distributed setups (e.g., sharded clusters or replica sets), network latency and bandwidth can impact performance.


2. Indexing
Proper Indexing: Indexes speed up query performance by allowing MongoDB to quickly locate documents. However, too many indexes can slow down write operations.
Index Selection: Using the right index for queries is crucial. MongoDB's query planner selects an index, but inefficient indexes can lead to poor performance.
Index Size: Large indexes consume more memory and disk space, which can impact performance.
Index Rebuilds: Rebuilding indexes can improve query performance if indexes become fragmented.


3. Query Design
Efficient Queries: Poorly designed queries (e.g., full collection scans) can significantly degrade performance.
Selectivity: Queries that return a large portion of the dataset are slower. Use filters to reduce the result set.
Projection: Retrieve only the fields you need instead of the entire document.
Aggregation Pipelines: Complex aggregations can be resource-intensive. Optimize pipeline stages and use indexes where possible.


4. Data Model
Schema Design: A well-designed schema can improve performance. For example:
Embedding vs. Referencing: Embedding related data in a single document can reduce the need for joins, but it can also lead to large documents.
Normalization vs. Denormalization: Denormalization can improve read performance but may increase write complexity.
Document Size: Large documents consume more memory and bandwidth. Avoid storing excessively large documents.
Data Distribution: In sharded clusters, uneven data distribution (e.g., due to a poor shard key) can lead to hotspots and performance bottlenecks.


6. Sharding and Replication
Sharding: Proper sharding distributes data and load across multiple servers, improving performance for large datasets. 
However, poor shard key selection can lead to imbalances.
Replication: Replica sets provide high availability and read scalability. Distributing read operations across secondaries can improve performance


7. Concurrency
Locking: MongoDB uses locks to manage concurrent operations. Long-running queries or high write loads can lead to contention and reduced performance.
WiredTiger Concurrency: WiredTiger uses document-level locking, which improves concurrency compared to the older MMAPv1 engine.


8. Caching
Working Set: MongoDB caches frequently accessed data and indexes in RAM. If the working set exceeds available RAM, performance will degrade due to increased disk I/O.
Filesystem Cache: The operating system's filesystem cache can also improve performance by caching frequently accessed data.


10. Application-Level Factors
Connection Management: Opening and closing connections frequently can add overhead. Use connection pooling to manage connections efficiently.
Batching: Batch write operations (e.g., insertMany) to reduce round-trips to the database.
Retry Logic: Implement retry logic for transient errors to avoid unnecessary load on the database.


11. Monitoring and Profiling
Monitoring: Use tools like MongoDB Atlas, mongostat, or mongotop to monitor performance metrics (e.g., CPU, memory, disk I/O).
Profiling: Enable the database profiler to identify slow queries and optimize them.





2Ô∏è‚É£. Capped Collections:

-> Similar to TTL index
-> A capped collection in MongoDB is a special type of collection with a fixed size and a circular behavior. Once the collection reaches its maximum size, it starts 
overwriting the oldest documents to make space for new ones. This makes capped collections ideal for use cases where you need to store a fixed amount of data and 
automatically discard older data.

Key Characteristics of Capped Collections:

1. Fixed Size:
You specify the maximum size (in bytes) when creating the collection.
Once the collection reaches this size, it starts overwriting the oldest documents.

2. Circular Behavior:
Capped collections behave like a circular buffer. When the collection is full, the oldest documents are removed to make room for new ones.

3. High Performance:
Capped collections are optimized for high-throughput operations. They support fast inserts and retrievals because they maintain documents in the order of insertion.

4. No Document Deletion:
You cannot manually delete documents from a capped collection (except by dropping the entire collection).

5. No Indexes (by default):
By default, capped collections only have an index on the _id field. You can add additional indexes, but this may impact performance.

6. Natural Order:
Documents in a capped collection are stored in the order of insertion. Queries return documents in this natural order unless you specify a sort.


Use Cases for Capped Collections:

1. Logging:
Store application logs where older logs are automatically discarded as new logs are added.

2. Caching:
Use capped collections to cache frequently accessed data with a fixed size.

3. Message Queues:
Implement simple message queues where older messages are automatically removed

‚úÖ Good For:

Real-time logs (e.g., store only the last 1000 logs).
Chat messages (e.g., keep only the last 500 messages in a chat).
Sensor data (e.g., keep only the last 10,000 temperature readings).

Keeps the collection size at 10MB.
Keeps a maximum of 1000 documents.
Old documents are automatically overwritten when the limit is reached.



How to Create a Capped Collection:
-> You can create a capped collection using the db.createCollection() method with the capped and size options.

Example:

db.createCollection("logs", {
  capped: true,
  size: 1048576, // Size in bytes (1 MB)
  max: 100      // Optional: Maximum number of documents, if you try to insert 101th document then it will throw error.
});

size: The maximum size of the collection in bytes.
max: (Optional) The maximum number of documents the collection can hold.


Behavior of Capped Collections:

Insertion:
New documents are appended to the collection.
If the collection is full, the oldest document is removed to make space.

Querying:
Queries return documents in the order of insertion (natural order).
You can use sort() to change the order, but this may impact performance.

Updates:
Updates are allowed, but you cannot increase the size of a document. If an update increases the document size, it will fail.

Deletion:
You cannot manually delete individual documents. The only way to remove documents is by dropping the entire collection.


Insert Logs:
db.logs.insertOne({ message: "App started "});

Find(): 
db.logs.find().sort({ $natural: -1 });


üîç TTL Index vs. Capped Collection ‚Äì When to Use?

Feature                  TTL Index                          Capped Collection
---------------------------------------------------------------------------------
Purpose                  Delete expired data               Store only recent data
How It Works?            Removes documents based on time  Keeps a fixed number of documents
Use Case                 Session data, OTPs, logs         Real-time logs, chat messages, sensor data
Data Deletion            Deletes documents permanently    Overwrites oldest data
Storage Control          No size control                  Fixed storage size




============================================================================================================================================================



3Ô∏è‚É£. Replica Sets:

Sharding and Replica Sets are two different concepts in MongoDB, and they serve different purposes ‚Äî but they can also work together in large-scale applications.


| Feature                  | Description                                                                                                 |
| ------------------------ | ----------------------------------------------------------------------------------------------------------- |
| üîÑ **Same Data**         | All members of the replica set have **copies of the same data**.                                            |
| üß† **Primary/Secondary** | One node is the **Primary** (handles writes); others are **Secondaries** (replicate data from the Primary). |
| üí• **Failover**          | If the primary fails, a secondary is automatically promoted to primary.                                     |
| üì¶ **Data Distribution** | **Not distributed**; all nodes have a **full copy** of the data.                                            |
| üí° **Use Case**          | Ensures your data is always available ‚Äî even during failures.                                               |


Each shard can be a **replica set** itself for high availability.  



1. Replica Set
A replica set is a group of MongoDB instances that maintain the same data set. 
It is used for high availability and data redundancy. 
If one instance fails, another can take over, ensuring that your database remains available.

How a Replica Set Works:
A replica set consists of multiple MongoDB instances (usually 3 or more).
One instance is the primary node, which handles all write operations and replicates data to the secondary nodes.
The secondary nodes are read-only and can be used for read operations or failover.

Example:
Imagine you have a replica set with 3 MongoDB instances:
Primary: Handles all write operations (e.g., inserting, updating, or deleting data).
Secondary 1: Replicates data from the primary and can handle read operations.
Secondary 2: Also replicates data from the primary and can handle read operations.
If the primary node fails, one of the secondary nodes is automatically elected as the new primary, ensuring continuous availability.
Key Benefits of a Replica Set:
High Availability: If the primary node fails, a secondary node takes over.
Data Redundancy: Data is replicated across multiple nodes, so no data is lost if one node fails.
Read Scalability: Read operations can be distributed across secondary nodes.


-> Replica set is something you create or manage as a database adminstrator or system adminstrator.
-> lets say we have client, either the mongo shell we are using or some native driver for node, PHP, C++ whatever it is.
Now we want to write some data to our databse for that we send out insert or update query to our mongoDB server which in the end talks to the primary node you could say.
Now important thing is a node here is mongodb server, so what we use thus far with this mongodb command gave us a node the only node we had.
so mongodb server is technically attached to that node but its a bit easier to understand it like this.
so we have that primary node and that is basically the setup we used for this entire course. we have our server which is one node.

Client (Shell, Driver)  ------>  MongoDB Server   ------>  Primary Node.

Now you can add more nodes, so called secondary nodes, so these are additional database servers which are all tied together in a so called replica set.
Now the idea here is that you always communicate with your primary node automatically.
if you send insert command to your connected mongo server it will automatically talk to primary node, but behind the scene prmary node will asynchrously replicate the
data on the secondary nodes  and asynchrously simply means that if you insert data  its not immediatly  written to the secondary nodes but relatively soon. so you have 
this replication of data.
why do we replicate data ?
well we do replicate data so that in this set up here, if we read data and for some reason, our primary node is offline then we can reach out to secondary node in 
replica set. so we get fault tolerance in here because  even if one of our server you could say goes down  we can talk to another.

                            Read                         Read                    Replica set
Client (Shell, Driver)  ---------->  MongoDB Server   ---------> | Primary Node --> secondary Node, secondary Node | 


============================================================================================================================================================



4Ô∏è‚É£. sharding (Horizontal Scaling):


Sharding and Replica Sets are two different concepts in MongoDB, and they serve different purposes ‚Äî but they can also work together in large-scale applications.


| Feature                  | Description                                                                                         |
| ------------------------ | --------------------------------------------------------------------------------------------------- |
| üß© **Data Partitioned**  | Data is **split across shards**, each holding only a **subset** of the data (based on a shard key). |
| üñ•Ô∏è **Shards**           | Each shard can be a **replica set** itself for high availability.                                   |
| üó∫Ô∏è **Config Servers**   | Store metadata and manage the cluster‚Äôs data distribution.                                          |
| üö¶ **mongos**            | Query router that directs operations to the correct shard.                                          |
| üì¶ **Data Distribution** | **Horizontally distributed** data (scale-out).                                                      |
| üí° **Use Case**          | For **very large datasets** or **high-throughput** systems that outgrow a single server's capacity. |



MongoDB is designed to handle large volumes of data and high traffic by distributing data across multiple servers, a process known as horizontal scaling or sharding.

What is Horizontal Scaling?
Horizontal scaling refers to adding more machines (servers) to your infrastructure to handle increased load or data, rather than upgrading a single server 
(vertical scaling). 

In MongoDB, this is achieved through sharding.

What is Sharding?
Sharding is a method of splitting and distributing data across multiple servers (called shards) to improve performance, storage capacity, and availability. 
Each shard is an independent database, and together, the shards make up a single logical database.

What is a Shard?

A shard is a single MongoDB instance (or a replica set) that stores a portion of the data. For example:
Shard 1: Stores orders from customers with last names starting with A‚ÄìM.
Shard 2: Stores orders from customers with last names starting with N‚ÄìZ.
Each shard is an independent MongoDB instance, but together, they form a single logical database.
The collection (orders in our example) is split across multiple shards, but it still appears as a single logical collection to the application.
The database (shop) remains the same, but its data is distributed across shards.



Example Scenario:
Imagine you have an e-commerce application with a MongoDB database called shop. Inside this database, you have a collection called orders that stores all customer orders.
Over time, the orders collection grows to billions of documents, and a single MongoDB server can no longer handle the load.
To solve this, you decide to shard the orders collection across multiple servers (shards).



How Sharding Works:

Shard Key:
You choose a shard key, which is a field in the documents that MongoDB uses to distribute data across shards.
In our example, the shard key could be the customer_last_name field.

Data Distribution:
MongoDB splits the data into chunks based on the shard key.

For example:
Chunk 1: customer_last_name values from A‚ÄìM ‚Üí stored in Shard 1.
Chunk 2: customer_last_name values from N‚ÄìZ ‚Üí stored in Shard 2.

Query Routing:
When a query is made, MongoDB's mongos (query router) determines which shard(s) contain the relevant data and routes the query accordingly.
For example, if you query for orders from customers with last names starting with "Smith," the query is sent to Shard 2.



How MongoDB Optimizes for Horizontal Scaling:

Automatic Data Distribution:
MongoDB automatically distributes data across shards based on a shard key (a field in the document used to partition data).
This ensures that data is evenly spread, preventing bottlenecks on a single server.

Scalability:
As your data grows, you can add more shards to the cluster, allowing MongoDB to handle larger datasets and higher query loads without degrading performance.

Load Balancing:
MongoDB's shard balancer ensures that data is evenly distributed across shards. If one shard becomes overloaded, the balancer redistributes chunks of data to other shards.

High Availability:
Sharding works in tandem with MongoDB's replication feature, where each shard can be a replica set (a group of servers that maintain the same data for redundancy). This ensures data availability even if a server fails.

Query Routing:  
MongoDB uses a mongos (MongoDB query router) to direct queries to the appropriate shard(s). This means applications don't need to know where the data is stored; the router handles it automatically.


Flexibility:
You can scale specific parts of your dataset independently. For example, if one collection is growing faster than others, you can allocate more resources to that 
specific shard.


Benefits of Horizontal Scaling in MongoDB:

Handles Large Datasets: Ideal for big data applications where the dataset exceeds the capacity of a single server.
Improved Performance: Distributes read/write operations across multiple servers, reducing latency.
Cost-Effective: Instead of investing in expensive, high-end hardware, you can use commodity servers.
Fault Tolerance: If one shard fails, the others continue to operate, ensuring system reliability


Use Cases for Sharding:

Applications with massive datasets (e.g., social media platforms, IoT systems, e-commerce sites).
High-throughput systems requiring low-latency responses.
Systems needing to scale dynamically as data grows.






























































































































































































