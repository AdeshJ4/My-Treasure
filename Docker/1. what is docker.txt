### What is Docker?
===================
-> Docker is a "platform" used to develop, ship, and run applications inside lightweight, portable "containers".
-> Docker is built on basic linux concepts.



### Why Use Docker?
===================
Docker helps developers package applications and their dependencies into containers, making them easy to run anywhereâ€”on a developerâ€™s laptop, a cloud server, 
or a production machine.

you can do both:
Run each application in a separate container âœ… (Best practice - Isolated environments (no conflicts))
Run all applications inside the same container âŒ (Not recommended - Dependency conflicts -  If one app crashes, everything goes down)


Problems Docker Solves
ðŸ›‘ 1. "It works on my machine, but not on yours!"
âœ… Solution: Docker ensures the app runs the same way everywhere by packaging everything (code, libraries, dependencies) inside a container.

ðŸ›‘ 2. Dependency Conflicts
âœ… Solution: Each container has its own isolated environment, so different apps with different dependencies can run on the same system without conflicts.
Example:
One app needs Python 3.8, another needs Python 3.10 â†’ No problem! Each runs in its own container.

ðŸ›‘ 3. Difficult Deployment
âœ… Solution: With Docker, you package everything once and run it anywhere. No need to manually install dependencies on every server.
Example:
A developer can build a Docker image locally and deploy it to AWS, Azure, or Google Cloud without modifications.

ðŸ›‘ 4. Wasting Resources with Virtual Machines (VMs)
âœ… Solution: Unlike VMs, "Docker shares the OS kernel", making it much lighter and faster.

|-------------------|--------------------------------------|----------------------------------|
| Feature           | Virtual Machine (VM)                 | Docker Container                 |
|-------------------|--------------------------------------|----------------------------------|
| Startup Time      | Minutes                              | Seconds                          |
| Size              | GBs (includes full OS)               | MBs (only app + dependencies)    |
| Performance       | Slower (needs full OS per VM)        | Faster (shares OS)               |
| Resource Usage    | Heavy (each VM needs CPU/RAM)        | Lightweight                      |
|-------------------|--------------------------------------|----------------------------------|

Would you like me to format it differently or add more details? ðŸš€


ðŸ›‘ 5. Scaling Apps is Hard
âœ… Solution: Docker works with tools like Kubernetes to scale applications easily. You can quickly add or remove containers as traffic increases or decreases.


When Should You Use Docker?
âœ” When you want consistent environments across development, testing, and production.
âœ” When deploying apps to cloud services like AWS, Azure, or Google Cloud.
âœ” When running microservices (breaking an app into smaller, independent services).
âœ” When you need fast, isolated environments without using heavy VMs.




Docker Architecture: 
====================

1. Docker Follows a Client-Server Architecture:
The Docker client communicates with the Docker daemon (server) using REST API, CLI, or SDKs.
The Docker Engine (daemon) runs in the background and handles building, running, and managing containers.

2. Containers Are Just Processes (But Isolated):
Containers are lightweight processes running on the host machine.
Unlike regular processes, each container has its own isolated filesystem, network, and dependencies.

3. Containers Share the Host OS Kernel:
All containers share the kernel of the host OS, not the full OS itself.
This is what makes Docker more lightweight than virtual machines.

4. Understanding the Kernel:
The kernel is the core of an OS, managing hardware resources like CPU, memory, and storage.
Different OS kernels provide different system APIs, which is why applications built for one OS cannot run on another without compatibility layers.

5. Why Can't We Run Windows Apps on Linux?
Applications interact with the OS kernel using system calls (APIs).
Since Windows and Linux have different kernels, applications are not natively compatible between them.

6. Running Containers on Different Operating Systems:
Linux Host â†’ Runs only Linux containers because they rely on the Linux kernel.
Windows Host â†’ Can run both Windows and Linux containers, but not at the same time.
Windows uses a lightweight VM (via WSL 2) to run Linux containers.
macOS Host â†’ Cannot run macOS containers (Apple does not support this).
Instead, Docker on macOS runs Linux containers inside a lightweight Linux VM (using HyperKit).




Virtual machine vs containers :
================================

### **Difference Between Virtual Machine (VM) and Docker Container**  

|----------------------|-------------------------------------------------|-----------------------------------------|
| Feature              | **Virtual Machine (VM)**                        | **Docker Container**                    |
|----------------------|-------------------------------------------------|-----------------------------------------|
| **Isolation**        | Fully isolated, runs its own OS                 | Isolated but shares the host OS kernel  |
| **OS Requirement**   | Each VM has a full OS (Linux, Windows, etc.)    | Uses the host OS kernel, no full OS     |
| **Kernel**           | Has its own kernel                              | Shares the host OS kernel               |
| **Startup Time**     | Slow (minutes)                                  | Fast (seconds)                          |
| **Size**             | Large (GBs, includes full OS)                   | Small (MBs, only app & dependencies)    |
| **Performance**      | Heavier, requires more CPU/RAM                  | Lightweight, uses fewer resources       |
| **Resource Usage**   | Requires dedicated CPU, RAM, and storage        | Shares system resources efficiently     |
| **Portability**      | Less portable (OS-dependent)                    | Highly portable (runs anywhere with Docker) |
| **Security**         | Stronger isolation (separate OS)                | Weaker isolation (shared kernel)        |
|----------------------|-------------------------------------------------|-----------------------------------------|

---

### **Can We Use Multiple VMs on the Same Machine?**  
âœ… **Yes, we can run multiple VMs on the same machine** using a **hypervisor** (like **VMware, VirtualBox, or Hyper-V**).  
- Each VM operates as a **completely independent machine**, running its own **OS, applications, and configurations**.  
- However, **running multiple VMs requires significant CPU, RAM, and storage**, since each VM includes a **full OS**.  

---

### **Docker vs. VMs: Which One to Use?**  
âœ” **Use VMs** when:  
   - You need **full OS isolation** (e.g., running Windows & Linux side by side).  
   - You require **high security** (stronger isolation between applications).  
   - Running legacy applications that need a **specific OS**.  

âœ” **Use Docker** when:  
   - You need **lightweight, fast, and portable** deployments.  
   - You want to **run multiple applications with minimal resource usage**.  
   - You need **scalability** (e.g., deploying microservices).  

Would you like a **diagram** to visualize how VMs and Docker work? ðŸš€





Key Concepts of Docker:
=======================

1ï¸âƒ£. Docker Containers:

-> Containers are lightweight, standalone, and portable.
-> They package everything needed to run an application, including code, dependencies, and configurations.
-> Containers are isolated from both the host system /Docker host and other containers. However, it does not run independently of the operating systemâ€”it shares the 
host OS kernel while maintaining its own user-space environment.
-> This prevents dependency conflicts between applications running on the same machine.
-> A container provides an isolated environment separate from your PC.
-> However, it does not run independently of the operating systemâ€”it shares the host OS kernel while maintaining its own user-space environment.
-> Docker container does not have its own OS. It only has its own isolated filesystem, network, and dependencies, but it still shares the host OS kernel.
-> Unlike virtual machines (VMs), containers do not include a full operating system. Each container shares the host OS kernel but runs in its own isolated user space.
-> This makes containers faster and more efficient than VMs.
-> Each container can have a different user-space environment (e.g., different dependencies, libraries, and tools).
However, all containers must be compatible with the host OS kernel.
-> A container is special kind of process  running on the host OS. Unlike regular processes, it has a dedicated filesystem, network, and isolated runtime 
environment provided by its Docker image thats why its special kind of process.

-> Docker host: A Docker host is the machine (computer or server) where Docker is installed and running. It is responsible for managing and running containers.
It could be:
âœ… Your PC or laptop (Windows, macOS, Linux), A Cloud server (AWS, Azure, Google Cloud), A Dedicated server running Linux or Windows
-> you cannot run Windows, Linux, and macOS containers all on the same Docker host at the same time. 
ex: 
1ï¸âƒ£. Linux Containers on Linux: 
If your Docker host is Linux, you can run Linux containers natively.
Windows containers will not run on a Linux host natively.
When you run a Linux container on a Linux host, it directly uses the host's Linux kernel.

2ï¸âƒ£. Windows Containers on Windows
If your Docker host is Windows, you can run Windows containers natively.
Linux containers can be run using WSL 2 (Windows Subsystem for Linux) or using Docker's LinuxKit VM.
If your "Docker host" is Windows, you cannot run Windows and Linux containers at the same time. But you can switch between them.
A Windows container must use a Windows host kernel (which is why you can't run a Windows container on a Linux host).

3ï¸âƒ£. macOS Containers?
macOS does not support native macOS containers because Apple does not provide a macOS container runtime.
On a Mac, Docker actually runs a Linux virtual machine under the hood (via Docker Desktop) to run Linux containers.
Windows containers cannot run on macOS.
macOS cannot run macOS containers, so it uses a Linux VM to run Linux containers.


|-------------------|-----------------------------|--------------------------------|
| Feature           | Virtual Machine (VM)        | Docker Container               |
|-------------------|-----------------------------|--------------------------------|
| Own OS            | âœ… Yes (Includes full OS)   | âŒ No (Shares host OS kernel) |
| Kernel            | Each VM has its own kernel  | Uses host OS kernel            |
| Size              | Large (GBs)                 | Small (MBs)                    |
| Startup Time      | Slow (minutes)              | Fast (seconds)                 |
|Performance        | Heavier (more resources)    | Lighter (faster)               |
|-------------------|-----------------------------|--------------------------------|
    




2ï¸âƒ£. Docker Images

-> A Docker image is a lightweight, standalone, and "executable package" that includes
âœ… Application code (e.g., a Node.js app, Python script, etc.)
âœ… Dependencies (e.g., libraries, frameworks, runtimes)
âœ… Configuration files (e.g., environment variables, ports, settings)
âœ… File system snapshot (includes all necessary files)
ðŸ‘‰ Think of a Docker image as a "blueprint" or "template" for creating containers.
-> You can create Docker images from a Dockerfile (a configuration file that defines the image) or pull them from a public or private registry like Docker Hub.
You pull or build a Docker image â†’ "docker pull node:18-alpine"
-> we can package our application into a image and run it virtually everywhere. this is beauty of docker.

Key Characteristics of a Docker Image:
1ï¸âƒ£ Immutable â€“ Once built, an image cannot be changed. If modifications are needed, a new image must be created.
2ï¸âƒ£ Layers & Caching â€“
Docker images are built using multiple layers (each command in a Dockerfile creates a new layer).
This makes building and reusing images faster (since unchanged layers are cached).
3ï¸âƒ£ Read-Only â€“
The base image remains unchanged when a container runs.
When a container starts, Docker adds a writable layer on top of the image to store temporary changes.
4ï¸âƒ£ Portable â€“
A Docker image can be shared and deployed anywhere (local, cloud, or Kubernetes).
It is stored in a Docker registry (e.g., Docker Hub, AWS ECR, GitHub Container Registry).



How Does a Docker Image Work?
1ï¸âƒ£ A Dockerfile defines how an image is built.
2ï¸âƒ£ The image is built using docker build.
3ï¸âƒ£ The image is stored in a registry (local or remote).
4ï¸âƒ£ When you run docker run, a container is created from the image.

How They Work Together (Image - Container)
1ï¸âƒ£ You pull or build a Docker image â†’ docker pull node:18-alpine
2ï¸âƒ£ You create and run a container from the image â†’ docker run -d --name my-container node:18-alpine
3ï¸âƒ£ The image stays the same, but containers can be stopped, started, or deleted without affecting the original image.



3. Dockerfile
-> A Dockerfile is a text document that contains "instructions on how to build a Docker image". It specifies the base image, installation steps, configuration, 
and any commands that should run when a container starts.

Example: Simple Dockerfile for a Node.js App
dockerfile:
---
# Use an official Node.js image as the base
FROM node:18-alpine

# Set the working directory inside the container
WORKDIR /app

# Copy package.json and install dependencies
COPY package.json ./
RUN npm install

# Copy the rest of the app
COPY . .

# Expose the application port
EXPOSE 3000

# Run the app
CMD ["node", "server.js"]
---

This file creates an image that packages a Node.js app with all dependencies.
You can build and run this image as a container.



### Difference Between `Dockerfile` and Docker Image**  

|------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| Feature    | Dockerfile                                                                  | Docker Image (Final Product)                                                |
|------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| Definition | A text file that contains step-by-step instructions to build a Docker image.| A built, executable package created from a `Dockerfile`.                    |
| Purpose    | Defines how the image should be created                                     | The actual package that contains everything needed to run an application.   |
| State      | Editable (you can modify the file).                                         | Immutable (once built, it cannot be changed directly).                      |
| Usage      | Used as input for `docker build` to create an image.                        | Used as input for `docker run` to create a container.                       |
| Storage    | A simple text file stored in a project directory.                           | Stored locally or in a container registry (Docker Hub, AWS ECR, etc.).      |
|------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------|

---

### **How They Work Together**
1ï¸âƒ£ **You write a `Dockerfile`** â†’ It contains instructions like `FROM`, `COPY`, `RUN`, `CMD`.  
2ï¸âƒ£ **You run `docker build`** â†’ This builds a **Docker Image** based on the `Dockerfile`.  
3ï¸âƒ£ **You run `docker run`** â†’ This starts a **Docker Container** from the built image.  

---

### **Example of a Dockerfile**
```
# Use an official Node.js base image
FROM node:18-alpine  

# Set working directory inside container
WORKDIR /app  

# Copy application files
COPY . .  

# Install dependencies
RUN npm install  

# Expose port 3000
EXPOSE 3000  

# Run the application
CMD ["node", "server.js"]  
```

- This `Dockerfile` defines how to build a Node.js app inside a container.  
- Running `docker build -t my-node-app .` creates a **Docker Image**.  




4ï¸âƒ£. Docker Engine : 

Docker Engine is the core component of Docker that allows you to build, run, and manage containers, images. 
It runs as a background service (daemon) on your system and provides a way to communicate with containers using a CLI (Command-Line Interface) or REST API.

Key Components of Docker Engine
1ï¸âƒ£ Docker Daemon (dockerd)
Runs in the background and manages images, containers, networks, and storage.
Listens for Docker API requests from the Docker CLI or other tools.

2ï¸âƒ£ Docker CLI (docker)
A command-line tool that allows users to interact with the Docker Daemon.
Example commands:
---
docker run nginx
docker ps
docker stop container_id
---

3ï¸âƒ£ REST API
Provides a way for applications and automation tools to communicate with Docker Daemon.

Docker Engine Manages the Following:
âœ… Containers â€“ Creating, running, stopping, and deleting containers.
âœ… Images â€“ Pulling, building, storing, and removing images.
âœ… Networks â€“ Managing container communication.
âœ… Storage â€“ Handling volumes and persistent data.


Docker Engine Workflow
ðŸ”¹ Step 1: The user runs a command using the Docker CLI (docker run nginx).
ðŸ”¹ Step 2: The CLI sends a request to the Docker Daemon.
ðŸ”¹ Step 3: The Daemon pulls the required Docker Image from Docker Hub (if not already available).
ðŸ”¹ Step 4: The Daemon creates and runs a container based on the image.
ðŸ”¹ Step 5: The user can manage the container (docker stop, docker restart, docker logs).


Types of Docker Engines
1ï¸âƒ£ Docker Engine - Community (CE) â†’ Free and open-source for developers and small teams.
2ï¸âƒ£ Docker Engine - Enterprise (EE) â†’ Paid version with extra security and support features for large businesses.




5ï¸âƒ£. Docker Hub:
-> Its a "cloud-based registry" that allows you to "store" and "share" "Docker images".
-> You can "pull images" from the Docker Hub or "push" your own images to it.
How Docker Hub Works:
1ï¸âƒ£ Pull an Image from Docker Hub
---
docker pull nginx
---
ðŸ”¹ This downloads the nginx image from Docker Hub to your local system.
2ï¸âƒ£ Run a Container from a Docker Hub Image
---
docker run -d -p 8080:80 nginx
---
ðŸ”¹ Runs an nginx web server using the pulled image.
3ï¸âƒ£ Push Your Own Image to Docker Hub

ðŸ”¹ Step 1: Log in to Docker Hub
---
docker login
---

ðŸ”¹ Step 2: Tag your image
---
docker tag my-app my-dockerhub-username/my-app:v1
---

ðŸ”¹ Step 3: Push the image to Docker Hub
---
docker push my-dockerhub-username/my-app:v1
---

4ï¸âƒ£ List All Images in Your Docker Hub Account
Visit: hub.docker.com
Log in and view your repositories.



6ï¸âƒ£. Docker Compose:
ðŸš€ Docker Compose is a tool that allows you to define and manage multi-container Docker applications using a simple YAML file (docker-compose.yml). 
Instead of running multiple docker run commands manually, Compose automates container creation, networking, and management.

Why Use Docker Compose?
âœ… Multiple Containers, One Command â€“ Easily manage multi-container applications (e.g., frontend + backend + database).
âœ… Simplifies Configuration â€“ Define everything in a single docker-compose.yml file.
âœ… Networking Made Easy â€“ Automatically creates a private network for containers to communicate.
âœ… Volume Management â€“ Handles persistent storage for databases and other services.
âœ… Scalable â€“ Easily scale containers up or down.
âœ… Environment Variables â€“ Store sensitive data like passwords separately using .env files.

What is a Multi-Container Docker Application?
ðŸš€ A multi-container Docker application is an application that runs multiple interconnected containers, each handling a specific part of the system. 
Instead of running everything in one container, you split your application into separate services, making it more scalable, modular, and maintainable.

Why Use Multi-Container Architecture?
âœ… Separation of Concerns â€“ Each container runs a specific service (backend, database, frontend, cache).
âœ… Scalability â€“ You can scale individual services independently (e.g., scale database separately from the backend).
âœ… Easy Maintenance & Updates â€“ Update or restart one service without affecting others.
âœ… Better Resource Management â€“ Services consume only the necessary resources instead of bloating a single container.
âœ… Microservices-Friendly â€“ Works well with microservices architectures.

Example: A Web Application with Multiple Containers
ðŸ›  Imagine a full-stack web application with the following components:
Frontend â€“ React application running in one container
Backend â€“ Node.js API running in another container
Database â€“ PostgreSQL running in a separate container
Cache â€“ Redis running in another container

Multi-Container Setup Using "docker-compose.yml": 
---
version: '3.8'

services:
  frontend:
    image: node:18
    container_name: react-app
    working_dir: /app
    volumes:
      - .:/app
    ports:
      - "3000:3000"
    command: npm start
    depends_on:
      - backend

  backend:
    image: node:18
    container_name: node-api
    working_dir: /api
    volumes:
      - .:/api
    ports:
      - "5000:5000"
    command: npm start
    depends_on:
      - database
      - cache

  database:
    image: postgres:latest
    container_name: postgres-db
    restart: always
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: mydb
    ports:
      - "5432:5432"

  cache:
    image: redis:latest
    container_name: redis-cache
    restart: always
    ports:
      - "6379:6379"
---

Running the Multi-Container Application
1ï¸âƒ£ Start all containers
---
docker-compose up -d
---
ðŸ”¹ This will start frontend, backend, database, and cache services in separate containers

2ï¸âƒ£ Check running containers
---
docker-compose ps
---

3ï¸âƒ£ Stop all containers
---
docker-compose down
---


Hereâ€™s your table in a proper format:  

|-----------------|-----------------------------------------|-------------------------------------------|
| Feature         | Single Container (Monolithic)           | Multi-Container (Microservices)           |
|-----------------|-----------------------------------------|-------------------------------------------|
| Scalability     | Hard to scale individual parts          | Easily scale each service separately      |
| Performance     | High resource usage                     | Optimized usage per service               |  
| Fault Isolation | If one part crashes, everything crashes | Other services continue running           |
| Maintainability | Difficult to update components          | Easier to update/fix individual parts     |
| Reusability     | Cannot reuse components easily          | Services can be reused elsewhere          |
|-----------------|-----------------------------------------|-------------------------------------------|





7. Docker Volumes ðŸ—„ï¸

A Docker Volume is a persistent storage (any data storage device that retains data after power to that device is shut off) mechanism used to store data outside of a 
containerâ€™s lifecycle. This means data is not lost when a container is stopped, removed, or recreated.


Why Use Docker Volumes?
âœ… Persistent Storage â€“ Keeps data even if the container is deleted.
âœ… Container Independence â€“ Data can be shared across multiple containers.
âœ… Performance Optimization â€“ Faster than bind mounts when dealing with large amounts of data.
âœ… Backup & Restore â€“ Easily backup or migrate data between containers








Development workFlow: 
======================
we take application, it does not matter what kind of application it is or how its build. 
we take application and "dockerize" it. which means we make small change so that it can be run by docker.
how, we just add "dockerfile" to it. "dockerfile" is a "plain text file" which includes instructions that docker uses to package our application into image.
also A Dockerfile is a text document that contains "instructions on how to build a Docker image". It specifies the base image, installation steps, configuration, 
and any commands that should run when a container starts.
This Image contain everything our application need to run. for ex. A cut down OS. A runtime environment (eg Node/python), Applications files, Third party libraries,
environment variables and so on. so we create a docker file and give it to docker for packaging our application into a image.

once we have image we tell the docker to start a container using that image.
container is a process but special kind of process because it has its own file system which is provided by image.
so our application is loaded inside a container and this is how we run our application locally on development machine.
and this is how we run our application locally on our development machine.
so instead of directly launching the application and running inside typical process we tell the docker to run it inside a container and in isolated environment.

Once we have this "image" we can push it to docker registry like docker hub, docker to docker hub is similar to git to github, its a storage for docker images that 
anyone can use. so once our "application image" is on docker hub then can pull it from any machine running docker. with this we can start the container in same way 
we do in development machine. we just tell docker to start a container using this image. 





Shradha Ma'am: 
==============

-> Manual process to install/set up dependencies for project, so manual error can occurred, 2nd error can be : different version of node or python can give errors 
like project needs node V16 but new member installed latests node v20, 3rd error can be commands to run applications like you wrote commands for running application
based on linux OS but new member have windows OS.

SO DOcker solves this issue.
Docker is a service which  helps you to create, build, update, destroy containers.
Containers take code and its dependencies and pack them in single unit/package and can be shared with fellow developers or can be deploy as a single package. 
Containers works on almost on every machine/OS.
We don't have to make any special changes in any other machine for running the container.
Containers are portable, we can share them from one machine to another.
Containers are lightweight.
Containers are instance of docker image.
Docker Image is a executable file. This file contain instructions to build a container. Using One Image we can build multiple containers. 
When we say that we are going to share a container with our fellow colleague/team then we are actually not sharing the container.
first we create the docker image of our application then that docker image is shared with others, then anyone can build container with this image inside any OS.
image is like a static screenshot/snapshot of what the code and dependencies is going to look like.


Steps to download docker : 
https://www.docker.com/products/docker-desktop/ 

open terminal and paste this : "docker pull hello-world"

to create a container from a image run : 

When someone says **"A container has its own environment separate from your PC"**, it means that:

### 1. **Isolation of Applications**  
A container runs **independently** of the operating system and software installed on your computer.  
- **Your PC (Host Machine)** might have different applications and libraries installed.  
- A **container** packages everything it needs to run (e.g., code, dependencies, libraries) in its own isolated environment.  
- This isolation prevents conflicts between applications and ensures that the containerized app behaves the same, no matter where it is deployed.

ðŸ‘‰ **Example**:  
- On your PC, you might have Python 3.10 installed.  
- A container can run an app that uses Python 3.7 without interfering with your PC's version of Python.  

### 2. **Filesystem Isolation**  
A container has its own **filesystem, processes, and networking** that are separate from the host machine.  
- Even though a container runs on the same kernel as the host, it uses its own virtual filesystem, which comes from the container image.  
- Changes made inside a container **do not affect your PC's filesystem** (unless explicitly configured to do so).  

ðŸ‘‰ **Example**:  
- If a container deletes files in `/app` inside its environment, the `/app` directory on your PC remains untouched.  

### 3. **Networking Isolation**  
Containers have their own virtual network interfaces and IP addresses.  
- They can communicate with each other or the outside world, but by default, they donâ€™t have direct access to the host machineâ€™s network unless configured.  

ðŸ‘‰ **Example**:  
- A container can run a web server on port `80` without affecting other web servers running on your PC.  

### 4. **Process Isolation**  
Containers run their own processes, which are separate from processes running on the host machine.  
- A container cannot see or interact with processes outside of its environment.  

ðŸ‘‰ **Example**:  
- If you stop or kill a process inside the container, it **wonâ€™t affect any processes on your PC**.  

---

### Why Does This Matter?  
- **Consistency**: Apps run the same way in development, testing, and production.  
- **Security**: Containers add a layer of isolation, reducing the risk of malicious software affecting the host.  
- **Efficiency**: Multiple containers can run on the same machine without interfering with each other.  

Would you like a hands-on example of creating a container to see this in action?





Basic Docker Commands: 


0]    docker version                  to check docker is installed or not.
i]    docker build -t <image-name>:   Build a Docker image from a Dockerfile in the current directory.
ii]   docker run <image-name>:        Run a container from a specified image
iii]  docker ps:                      List running containers
iv]   docker stop <container-id>:     Stop a running container
v]    docker rm <container-id>:       Remove a stopped container
vi]   docker images:                  List available images
vii]  docker pull <image-name>:       Download an image from a registry like Docker Hub.


Example Workflow:
Write a Dockerfile: Define how the container should be built (e.g., which base image to use, what dependencies to install, etc.).

Build the Image: Run docker build -t my-app . to create a Docker image based on the Dockerfile.

Run a Container: Use docker run to start a container from the image. For example, docker run -d -p 80:80 my-app will run the application in the container and map port 80 to the host.

Manage Containers: Use docker ps, docker stop, and docker rm to manage your running containers



Imagine you're building a cool project, like a toy robot. You need a lot of parts to make it work â€” wires, motors, a power source, 
sensors, and even a special controller. All these parts need to fit together perfectly, and they have to work on the same 
"blueprint" so your robot behaves just as you want.

Now, let's say you want to share this robot with your friend who lives far away. 
You send all the parts, but when your friend tries to put them together, something doesnâ€™t work right. 
Maybe the power source doesnâ€™t match, or the wires donâ€™t fit. Your friend might have a slightly different setup than yours, 
and now the robot doesnâ€™t work the way you intended. Frustrating, right?

This is the same problem people have with software! When we create programs, they rely on specific "parts" or environments, 
like certain versions of libraries, configurations, or tools. If someone tries to run that program on a slightly different setup, 
it might not work or might behave differently. 



### Why Docker is Like a Magic Box for Programs

Docker helps solve this problem by creating a "container." Think of a container as a magic box where you put your program and 
everything it needs to run, perfectly packaged together. 
This includes the specific versions of libraries, settings, and tools it relies on. 

When your friend gets the container (the magic box) and opens it, everything inside is exactly how it should be, 
and the program runs perfectly, no matter where it is. So, Docker containers make sure that programs can run anywhere without 
surprises!




### Why Do We Use Docker?

1. **Consistency**: Docker makes sure that software works the same way on any computer, whether itâ€™s your laptop, your friend's 
computer, or a big server. This solves the "it works on my computer" problem.
  
2. **Efficiency**: Docker containers are light and fast. Instead of needing an entire computer for each program, multiple Docker 
containers can run side by side on one machine, each with its own "environment." Itâ€™s like sharing one big house but having separate 
rooms with everything each person needs.
  
3. **Easier Collaboration**: Developers can share containers with each other, so everyone is working with the exact same setup. 
This helps teams avoid unexpected issues and speeds up development.

4. **Quick Setup**: If you need a specific environment, you donâ€™t have to install a bunch of things manually. You just grab a Docker 
container with everything pre-set up.




### What Problems Docker Solved

Before Docker, developers had to spend a lot of time setting up their environments and making sure everything matched perfectly. 
If a developer wrote code on one computer, it might not work on another because of tiny differences. 
Docker solved this by making it easy to package programs with all their dependencies, so they run the same everywhere.

In short, **Docker makes sure that programs work consistently and easily, no matter where you run them.**











Docker In Action: 
=================

Typical development workflow: 

Instructions if we are not using docker : 

1. Start the machine which will start OS of that machine
2. Then we have to install specific versions of node/python supported by project.
3. Then we have to copy all the file and have to make chnges in file structure.
4. Then Run the code.


we can write all these above instructions in docker file and let docker package our application.

create a hello-docker folder and open it inside vs code.
create a "app.js" file and add js code.
create a "Dockerfile", this file don't have extension.
typically we start from base image.

What is a Base Image?
A base image is like a starting point for your Docker container.
It contains a minimal operating system or runtime environment (e.g., Ubuntu, Node.js, Python).
Base images can be official ones from Docker Hub (like ubuntu, node, or alpine) or custom-made by others

Why Use a Base Image?
Instead of creating everything from scratch, you start with a pre-built environment
For example:  FROM node:16

Adding Additional Files
After selecting the base image, you add files or install additional software to customize the environment.
Example: 
FROM node:16
WORKDIR /app
COPY . .
RUN npm install

explanation:
FROM node:16 â€“ Start from Node.js 16 base image.
WORKDIR /app â€“ Set the working directory inside the container.
COPY . . â€“ Copy files from your project folder to the container.
RUN npm install â€“ Install dependencies inside the container





node image is built on top of different distributions of linux, so linux have different distributions or different flavours use for different purposes.
eg: FROM node:alpine
the size of image that we are gonna download and build on top of is going to be small.
then we copy our application files for that we use COPY command, we copy all the files in the current directory (.) into a app directory (/app) into that image. 
that image have file system and in that file system we are gonna create a directory called "app".
ex: 
FROM node:alpine
COPY . /app
we use "CMD" command to execute command, what command should we execute here ? "node /app/app.js".
ex:
FROM node:alpine
COPY . /app
CMD node /app/app.js
alternatively we can set current working directory, when we use this instructions all the following instructions assume you are currently inside a app directory.
ex: 
FROM node:alpine
COPY . /app
WORKDIR /app
CMD node app.js

So these instructions clearly document our deployment process.
now open terminal to tell docker to package our application
type > docker build
we have to give our image a tag, tag to identify, then we have to specify where docker can find this file. 
terminal path: PS C:\Users\Anurag\Desktop\hello-world>
so we are inside "hello-world" directory and our dockerfile is right here. so we use dot (.) to reference a current directory.
type > docker build -t hello-docker .
expected result: [+] Building 10.7s (8/8) FINISHED     

to see all the images : 
PS Desktop\hello-world> docker images
REPOSITORY     TAG       IMAGE ID       CREATED         SIZE
hello-docker   latest    b730f6678a28   3 minutes ago   228MB
hello-world    latest    5b3cc85e16e3   20 months ago   24.4kB


since we use node from linux alpine we end up with only 228MB of data.
so this image contain node, alpine, and our files 

if you use different version of node based on different version of linux we will end up wth larger image and when deploying that image 
we have to transfer the image from one computer to another.
We build the image and that image can be run on any computer running docker.
PS C:\Users\Anurag.LAPTOP-1QDKUF7J\Desktop\hello-world> docker run hello-docker
Hello Docker

i can go ahead and publish the image on docker hub so anyone can install this image.then i can go on another machine like test or production 
machine and pull and run the image. 






Linux Distributions: 

also called linus distros.

linux is opens sourse platform.
